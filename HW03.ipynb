{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.7 (default, May  6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)]\n",
      "pandas version: 1.0.5\n",
      "matplotlib version: 3.2.2\n",
      "NumPy version: 1.18.1\n",
      "SciPy version: 1.4.1\n",
      "IPython version: 7.14.0\n",
      "scikit-learn version: 0.23.1\n",
      "tensorflow version: 2.2.0\n",
      "keras version: 2.4.2\n",
      "-------------------------\n",
      " Volume in drive C has no label.\r\n",
      " Volume Serial Number is 9E12-1BF2\r\n",
      "\r\n",
      " Directory of C:\\Users\\Odin\\PycharmProjects\\Machine-Learning-HWs\\HW03\\input\r\n",
      "\r\n",
      "06/14/2020  12:22 PM    <DIR>          .\r\n",
      "06/14/2020  12:22 PM    <DIR>          ..\r\n",
      "12/11/2019  03:17 AM             3,258 gender_submission.csv\r\n",
      "12/11/2019  03:17 AM            28,629 test.csv\r\n",
      "12/11/2019  03:17 AM            61,194 train.csv\r\n",
      "               3 File(s)         93,081 bytes\r\n",
      "               2 Dir(s)  19,517,874,176 bytes free\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Import Libraries\n",
    "\n",
    "\n",
    "#load packages\n",
    "import sys #access to system parameters https://docs.python.org/3/library/sys.html\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "\n",
    "import pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "import matplotlib #collection of functions for scientific and publication-ready visualization\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "\n",
    "import numpy as np #foundational package for scientific computing\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "\n",
    "import scipy as sp #collection of functions for scientific computing and advance mathematics\n",
    "print(\"SciPy version: {}\". format(sp.__version__)) \n",
    "\n",
    "import IPython\n",
    "from IPython import display #pretty printing of dataframes in Jupyter notebook\n",
    "print(\"IPython version: {}\". format(IPython.__version__)) \n",
    "\n",
    "import sklearn #collection of machine learning algorithms\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "import tensorflow as tf #collection of machine learning algorithms\n",
    "print(\"tensorflow version: {}\". format(tf.__version__))\n",
    "\n",
    "import keras  #collection of machine learning algorithms\n",
    "print(\"keras version: {}\". format(keras.__version__))\n",
    "\n",
    "\n",
    "\n",
    "#misc libraries\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('-'*25)\n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"dir\", \"input\",],shell=True).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common Model Algorithms\n",
    "from sklearn import svm, tree,cluster, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process ,decomposition\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from keras.models import Sequential # intitialize the ANN\n",
    "from keras.layers import Dense ,Dropout    # create layers \n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold , cross_val_score\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "import pandas\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#Configure Visualization Defaults\n",
    "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "# sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8\n",
    "\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>386</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Davies, Mr. Charles Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S.O.C. 14879</td>\n",
       "      <td>73.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>596</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Van Impe, Mr. Jean Baptiste</td>\n",
       "      <td>male</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>345773</td>\n",
       "      <td>24.1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>295</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mineff, Mr. Ivan</td>\n",
       "      <td>male</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349233</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>676</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Edvardsson, Mr. Gustaf Hjalmar</td>\n",
       "      <td>male</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349912</td>\n",
       "      <td>7.7750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>813</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Slemen, Mr. Richard James</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28206</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>877</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Gustafsson, Mr. Alfred Ossian</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7534</td>\n",
       "      <td>9.8458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>698</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Mullens, Miss. Katherine \"Katie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35852</td>\n",
       "      <td>7.7333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Olsen, Mr. Karl Siegwart Andreas</td>\n",
       "      <td>male</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4579</td>\n",
       "      <td>8.4042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Collyer, Miss. Marjorie \"Lottie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>C.A. 31921</td>\n",
       "      <td>26.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>208</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Albimona, Mr. Nassef Cassem</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2699</td>\n",
       "      <td>18.7875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                              Name     Sex  \\\n",
       "385          386         0       2         Davies, Mr. Charles Henry    male   \n",
       "595          596         0       3       Van Impe, Mr. Jean Baptiste    male   \n",
       "294          295         0       3                  Mineff, Mr. Ivan    male   \n",
       "675          676         0       3    Edvardsson, Mr. Gustaf Hjalmar    male   \n",
       "812          813         0       2         Slemen, Mr. Richard James    male   \n",
       "876          877         0       3     Gustafsson, Mr. Alfred Ossian    male   \n",
       "697          698         1       3  Mullens, Miss. Katherine \"Katie\"  female   \n",
       "197          198         0       3  Olsen, Mr. Karl Siegwart Andreas    male   \n",
       "237          238         1       2  Collyer, Miss. Marjorie \"Lottie\"  female   \n",
       "207          208         1       3       Albimona, Mr. Nassef Cassem    male   \n",
       "\n",
       "      Age  SibSp  Parch        Ticket     Fare Cabin Embarked  \n",
       "385  18.0      0      0  S.O.C. 14879  73.5000   NaN        S  \n",
       "595  36.0      1      1        345773  24.1500   NaN        S  \n",
       "294  24.0      0      0        349233   7.8958   NaN        S  \n",
       "675  18.0      0      0        349912   7.7750   NaN        S  \n",
       "812  35.0      0      0         28206  10.5000   NaN        S  \n",
       "876  20.0      0      0          7534   9.8458   NaN        S  \n",
       "697   NaN      0      0         35852   7.7333   NaN        Q  \n",
       "197  42.0      0      1          4579   8.4042   NaN        S  \n",
       "237   8.0      0      2    C.A. 31921  26.2500   NaN        S  \n",
       "207  26.0      0      0          2699  18.7875   NaN        C  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "data_raw = pd.read_csv('input/train.csv')\n",
    "\n",
    "\n",
    "#a dataset should be broken into 3 splits: train, test, and (final) validation\n",
    "#the test file provided is the validation file for competition submission\n",
    "#we will split the train set into train and test data in future sections\n",
    "data_val  = pd.read_csv('input/test.csv')\n",
    "\n",
    "\n",
    "#to play with our data we'll create a copy\n",
    "#remember python assignment or equal passes by reference vs values, so we use the copy function: https://stackoverflow.com/questions/46327494/python-pandas-dataframe-copydeep-false-vs-copydeep-true-vs\n",
    "data1 = data_raw.copy(deep = True)\n",
    "\n",
    "#however passing by reference is convenient, because we can clean both datasets at once\n",
    "data_cleaner = [data1, data_val]\n",
    "\n",
    "\n",
    "#preview data\n",
    "print (data_raw.info())\n",
    "#data_raw.head()\n",
    "#data_raw.tail() \n",
    "data_raw.sample(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing: ## Correcting\n",
    "## Completing\n",
    "## Creating\n",
    "## Converting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns with null values:\n",
      " PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "----------\n",
      "Test/Validation columns with null values:\n",
      " PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>204</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>891</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Karun, Miss. Manca</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B96 B98</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PassengerId    Survived      Pclass                Name   Sex  \\\n",
       "count    891.000000  891.000000  891.000000                 891   891   \n",
       "unique          NaN         NaN         NaN                 891     2   \n",
       "top             NaN         NaN         NaN  Karun, Miss. Manca  male   \n",
       "freq            NaN         NaN         NaN                   1   577   \n",
       "mean     446.000000    0.383838    2.308642                 NaN   NaN   \n",
       "std      257.353842    0.486592    0.836071                 NaN   NaN   \n",
       "min        1.000000    0.000000    1.000000                 NaN   NaN   \n",
       "25%      223.500000    0.000000    2.000000                 NaN   NaN   \n",
       "50%      446.000000    0.000000    3.000000                 NaN   NaN   \n",
       "75%      668.500000    1.000000    3.000000                 NaN   NaN   \n",
       "max      891.000000    1.000000    3.000000                 NaN   NaN   \n",
       "\n",
       "               Age       SibSp       Parch Ticket        Fare    Cabin  \\\n",
       "count   714.000000  891.000000  891.000000    891  891.000000      204   \n",
       "unique         NaN         NaN         NaN    681         NaN      147   \n",
       "top            NaN         NaN         NaN   1601         NaN  B96 B98   \n",
       "freq           NaN         NaN         NaN      7         NaN        4   \n",
       "mean     29.699118    0.523008    0.381594    NaN   32.204208      NaN   \n",
       "std      14.526497    1.102743    0.806057    NaN   49.693429      NaN   \n",
       "min       0.420000    0.000000    0.000000    NaN    0.000000      NaN   \n",
       "25%      20.125000    0.000000    0.000000    NaN    7.910400      NaN   \n",
       "50%      28.000000    0.000000    0.000000    NaN   14.454200      NaN   \n",
       "75%      38.000000    1.000000    0.000000    NaN   31.000000      NaN   \n",
       "max      80.000000    8.000000    6.000000    NaN  512.329200      NaN   \n",
       "\n",
       "       Embarked  \n",
       "count       889  \n",
       "unique        3  \n",
       "top           S  \n",
       "freq        644  \n",
       "mean        NaN  \n",
       "std         NaN  \n",
       "min         NaN  \n",
       "25%         NaN  \n",
       "50%         NaN  \n",
       "75%         NaN  \n",
       "max         NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train columns with null values:\\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values:\\n', data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "data_raw.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived      0\n",
      "Pclass        0\n",
      "Name          0\n",
      "Sex           0\n",
      "Age         177\n",
      "SibSp         0\n",
      "Parch         0\n",
      "Fare          0\n",
      "Embarked      0\n",
      "dtype: int64\n",
      "----------\n",
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for dataset in data_cleaner:    \n",
    "    #complete missing age with median\n",
    "#     dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n",
    "\n",
    "    #complete embarked with mode\n",
    "    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n",
    "\n",
    "    #complete missing fare with median\n",
    "    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n",
    "    \n",
    "#delete the cabin feature/column and others previously stated to exclude in train dataset\n",
    "drop_column = ['PassengerId','Cabin', 'Ticket']\n",
    "data1.drop(drop_column, axis=1, inplace = True)\n",
    "\n",
    "print(data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print(data_val.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr        517\n",
      "Miss      182\n",
      "Mrs       125\n",
      "Master     40\n",
      "Misc       27\n",
      "Name: Title, dtype: int64\n",
      "----------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 13 columns):\n",
      " #   Column      Non-Null Count  Dtype   \n",
      "---  ------      --------------  -----   \n",
      " 0   Survived    891 non-null    int64   \n",
      " 1   Pclass      891 non-null    int64   \n",
      " 2   Name        891 non-null    object  \n",
      " 3   Sex         891 non-null    object  \n",
      " 4   Age         714 non-null    float64 \n",
      " 5   SibSp       891 non-null    int64   \n",
      " 6   Parch       891 non-null    int64   \n",
      " 7   Fare        891 non-null    float64 \n",
      " 8   Embarked    891 non-null    object  \n",
      " 9   FamilySize  891 non-null    int64   \n",
      " 10  IsAlone     891 non-null    int64   \n",
      " 11  Title       891 non-null    object  \n",
      " 12  FareBin     891 non-null    category\n",
      "dtypes: category(1), float64(2), int64(6), object(4)\n",
      "memory usage: 84.7+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   PassengerId  418 non-null    int64   \n",
      " 1   Pclass       418 non-null    int64   \n",
      " 2   Name         418 non-null    object  \n",
      " 3   Sex          418 non-null    object  \n",
      " 4   Age          332 non-null    float64 \n",
      " 5   SibSp        418 non-null    int64   \n",
      " 6   Parch        418 non-null    int64   \n",
      " 7   Ticket       418 non-null    object  \n",
      " 8   Fare         418 non-null    float64 \n",
      " 9   Cabin        91 non-null     object  \n",
      " 10  Embarked     418 non-null    object  \n",
      " 11  FamilySize   418 non-null    int64   \n",
      " 12  IsAlone      418 non-null    int64   \n",
      " 13  Title        418 non-null    object  \n",
      " 14  FareBin      418 non-null    category\n",
      "dtypes: category(1), float64(2), int64(6), object(6)\n",
      "memory usage: 46.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>Title</th>\n",
       "      <th>FareBin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Lam, Mr. Len</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.4958</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr</td>\n",
       "      <td>(31.0, 512.329]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Wick, Mrs. George Dennick (Mary Hitchcock)</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>164.8667</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>(31.0, 512.329]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Carter, Mrs. Ernest Courtenay (Lilian Hughes)</td>\n",
       "      <td>female</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>(14.454, 31.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sawyer, Mr. Frederick Charles</td>\n",
       "      <td>male</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr</td>\n",
       "      <td>(7.91, 14.454]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Persson, Mr. Ernst Ulrik</td>\n",
       "      <td>male</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7750</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr</td>\n",
       "      <td>(-0.001, 7.91]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Hood, Mr. Ambrose Jr</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73.5000</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr</td>\n",
       "      <td>(31.0, 512.329]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Bourke, Mrs. John (Catherine)</td>\n",
       "      <td>female</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.5000</td>\n",
       "      <td>Q</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>(14.454, 31.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Svensson, Mr. Johan</td>\n",
       "      <td>male</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7750</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr</td>\n",
       "      <td>(-0.001, 7.91]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Taussig, Mrs. Emil (Tillie Mandelbaum)</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79.6500</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>(31.0, 512.329]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Turpin, Mrs. William John Robert (Dorothy Ann ...</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>(14.454, 31.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass                                               Name  \\\n",
       "826         0       3                                       Lam, Mr. Len   \n",
       "856         1       1         Wick, Mrs. George Dennick (Mary Hitchcock)   \n",
       "854         0       2      Carter, Mrs. Ernest Courtenay (Lilian Hughes)   \n",
       "676         0       3                      Sawyer, Mr. Frederick Charles   \n",
       "267         1       3                           Persson, Mr. Ernst Ulrik   \n",
       "72          0       2                               Hood, Mr. Ambrose Jr   \n",
       "657         0       3                      Bourke, Mrs. John (Catherine)   \n",
       "851         0       3                                Svensson, Mr. Johan   \n",
       "558         1       1             Taussig, Mrs. Emil (Tillie Mandelbaum)   \n",
       "41          0       2  Turpin, Mrs. William John Robert (Dorothy Ann ...   \n",
       "\n",
       "        Sex   Age  SibSp  Parch      Fare Embarked  FamilySize  IsAlone Title  \\\n",
       "826    male   NaN      0      0   56.4958        S           1        1    Mr   \n",
       "856  female  45.0      1      1  164.8667        S           3        0   Mrs   \n",
       "854  female  44.0      1      0   26.0000        S           2        0   Mrs   \n",
       "676    male  24.5      0      0    8.0500        S           1        1    Mr   \n",
       "267    male  25.0      1      0    7.7750        S           2        0    Mr   \n",
       "72     male  21.0      0      0   73.5000        S           1        1    Mr   \n",
       "657  female  32.0      1      1   15.5000        Q           3        0   Mrs   \n",
       "851    male  74.0      0      0    7.7750        S           1        1    Mr   \n",
       "558  female  39.0      1      1   79.6500        S           3        0   Mrs   \n",
       "41   female  27.0      1      0   21.0000        S           2        0   Mrs   \n",
       "\n",
       "             FareBin  \n",
       "826  (31.0, 512.329]  \n",
       "856  (31.0, 512.329]  \n",
       "854   (14.454, 31.0]  \n",
       "676   (7.91, 14.454]  \n",
       "267   (-0.001, 7.91]  \n",
       "72   (31.0, 512.329]  \n",
       "657   (14.454, 31.0]  \n",
       "851   (-0.001, 7.91]  \n",
       "558  (31.0, 512.329]  \n",
       "41    (14.454, 31.0]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###CREATE: Feature Engineering for train and test/validation dataset\n",
    "for dataset in data_cleaner:    \n",
    "    #Discrete variables\n",
    "    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n",
    "\n",
    "    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n",
    "    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n",
    "\n",
    "    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n",
    "    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "\n",
    "\n",
    "    #Continuous variable bins; qcut vs cut: https://stackoverflow.com/questions/30211923/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n",
    "    #Fare Bins/Buckets using qcut or frequency bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html\n",
    "    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n",
    "\n",
    "    #Age Bins/Buckets using cut or value bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n",
    "#     dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "stat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http://nicholasjjackson.com/2012/03/08/sample-size-is-10-a-magic-number/\n",
    "title_names = (data1['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n",
    "\n",
    "#apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https://community.modeanalytics.com/python/tutorial/pandas-groupby-and-python-lambda-functions/\n",
    "data1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n",
    "print(data1['Title'].value_counts())\n",
    "print(\"-\"*10)\n",
    "\n",
    "\n",
    "#preview data again\n",
    "data1.info()\n",
    "data_val.info()\n",
    "data1.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that instead of using Title, we should use its corresponding dummy variables \n",
    "label = LabelEncoder()\n",
    "for dataset in data_cleaner:\n",
    "    df_sub1 = dataset[['Age','Pclass','Sex','SibSp','FareBin','Title','IsAlone','FamilySize','Embarked']]\n",
    "    for col in ['Sex','Title','FareBin','Embarked'] :\n",
    "        df_sub1[col] = label.fit_transform(df_sub1[col])\n",
    "\n",
    "    df_sub1.head()\n",
    "\n",
    "    X_train  = df_sub1.dropna().drop('Age', axis=1)\n",
    "    y_train  = dataset['Age'].dropna()\n",
    "    X_test = df_sub1.loc[np.isnan(dataset.Age)].drop('Age', axis=1)\n",
    "\n",
    "    regressor = RandomForestRegressor(n_estimators = 300)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    y_pred = np.round(regressor.predict(X_test),1)\n",
    "    dataset.Age.loc[dataset.Age.isnull()] = y_pred\n",
    "\n",
    "    dataset.Age.isnull().sum(axis=0) # no more NAN now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in data_cleaner:    \n",
    "    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X Y:  ['Survived', 'Sex', 'Pclass', 'Embarked', 'Title', 'SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] \n",
      "\n",
      "Bin X Y:  ['Survived', 'Sex_Code', 'Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code'] \n",
      "\n",
      "Dummy X Y:  ['Survived', 'Pclass', 'SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Title_Master', 'Title_Misc', 'Title_Miss', 'Title_Mr', 'Title_Mrs'] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Title_Master</th>\n",
       "      <th>Title_Misc</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  SibSp  Parch   Age     Fare  FamilySize  IsAlone  Sex_female  \\\n",
       "0       3      1      0  22.0   7.2500           2        0           0   \n",
       "1       1      1      0  38.0  71.2833           2        0           1   \n",
       "2       3      0      0  26.0   7.9250           1        1           1   \n",
       "3       1      1      0  35.0  53.1000           2        0           1   \n",
       "4       3      0      0  35.0   8.0500           1        1           0   \n",
       "\n",
       "   Sex_male  Embarked_C  Embarked_Q  Embarked_S  Title_Master  Title_Misc  \\\n",
       "0         1           0           0           1             0           0   \n",
       "1         0           1           0           0             0           0   \n",
       "2         0           0           0           1             0           0   \n",
       "3         0           0           0           1             0           0   \n",
       "4         1           0           0           1             0           0   \n",
       "\n",
       "   Title_Miss  Title_Mr  Title_Mrs  \n",
       "0           0         1          0  \n",
       "1           0         0          1  \n",
       "2           1         0          0  \n",
       "3           0         0          1  \n",
       "4           0         1          0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n",
    "\n",
    "#code categorical data\n",
    "label = LabelEncoder()\n",
    "for dataset in data_cleaner:    \n",
    "    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n",
    "    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n",
    "    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n",
    "    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n",
    "    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n",
    "\n",
    "\n",
    "#define y variable aka target/outcome\n",
    "Target = ['Survived']\n",
    "\n",
    "#define x variables for original features aka feature selection\n",
    "data1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name/values for charts\n",
    "data1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\n",
    "data1_xy =  Target + data1_x\n",
    "print('Original X Y: ', data1_xy, '\\n')\n",
    "\n",
    "\n",
    "#define x variables for original w/bin features to remove continuous variables\n",
    "data1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\n",
    "data1_xy_bin = Target + data1_x_bin\n",
    "print('Bin X Y: ', data1_xy_bin, '\\n')\n",
    "\n",
    "\n",
    "#define x and y variables for dummy features original\n",
    "data1_dummy = pd.get_dummies(data1[data1_x])\n",
    "data1_x_dummy = data1_dummy.columns.tolist()\n",
    "data1_xy_dummy = Target + data1_x_dummy\n",
    "print('Dummy X Y: ', data1_xy_dummy, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "data1_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns with null values: \n",
      " Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age              0\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Fare             0\n",
      "Embarked         0\n",
      "FamilySize       0\n",
      "IsAlone          0\n",
      "Title            0\n",
      "FareBin          0\n",
      "AgeBin           0\n",
      "Sex_Code         0\n",
      "Embarked_Code    0\n",
      "Title_Code       0\n",
      "AgeBin_Code      0\n",
      "FareBin_Code     0\n",
      "dtype: int64\n",
      "----------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 19 columns):\n",
      " #   Column         Non-Null Count  Dtype   \n",
      "---  ------         --------------  -----   \n",
      " 0   Survived       891 non-null    int64   \n",
      " 1   Pclass         891 non-null    int64   \n",
      " 2   Name           891 non-null    object  \n",
      " 3   Sex            891 non-null    object  \n",
      " 4   Age            891 non-null    float64 \n",
      " 5   SibSp          891 non-null    int64   \n",
      " 6   Parch          891 non-null    int64   \n",
      " 7   Fare           891 non-null    float64 \n",
      " 8   Embarked       891 non-null    object  \n",
      " 9   FamilySize     891 non-null    int64   \n",
      " 10  IsAlone        891 non-null    int64   \n",
      " 11  Title          891 non-null    object  \n",
      " 12  FareBin        891 non-null    category\n",
      " 13  AgeBin         891 non-null    category\n",
      " 14  Sex_Code       891 non-null    int32   \n",
      " 15  Embarked_Code  891 non-null    int32   \n",
      " 16  Title_Code     891 non-null    int32   \n",
      " 17  AgeBin_Code    891 non-null    int32   \n",
      " 18  FareBin_Code   891 non-null    int32   \n",
      "dtypes: category(2), float64(2), int32(5), int64(6), object(4)\n",
      "memory usage: 103.3+ KB\n",
      "None\n",
      "----------\n",
      "Test/Validation columns with null values: \n",
      " PassengerId        0\n",
      "Pclass             0\n",
      "Name               0\n",
      "Sex                0\n",
      "Age                0\n",
      "SibSp              0\n",
      "Parch              0\n",
      "Ticket             0\n",
      "Fare               0\n",
      "Cabin            327\n",
      "Embarked           0\n",
      "FamilySize         0\n",
      "IsAlone            0\n",
      "Title              0\n",
      "FareBin            0\n",
      "AgeBin             0\n",
      "Sex_Code           0\n",
      "Embarked_Code      0\n",
      "Title_Code         0\n",
      "AgeBin_Code        0\n",
      "FareBin_Code       0\n",
      "dtype: int64\n",
      "----------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype   \n",
      "---  ------         --------------  -----   \n",
      " 0   PassengerId    418 non-null    int64   \n",
      " 1   Pclass         418 non-null    int64   \n",
      " 2   Name           418 non-null    object  \n",
      " 3   Sex            418 non-null    object  \n",
      " 4   Age            418 non-null    float64 \n",
      " 5   SibSp          418 non-null    int64   \n",
      " 6   Parch          418 non-null    int64   \n",
      " 7   Ticket         418 non-null    object  \n",
      " 8   Fare           418 non-null    float64 \n",
      " 9   Cabin          91 non-null     object  \n",
      " 10  Embarked       418 non-null    object  \n",
      " 11  FamilySize     418 non-null    int64   \n",
      " 12  IsAlone        418 non-null    int64   \n",
      " 13  Title          418 non-null    object  \n",
      " 14  FareBin        418 non-null    category\n",
      " 15  AgeBin         418 non-null    category\n",
      " 16  Sex_Code       418 non-null    int32   \n",
      " 17  Embarked_Code  418 non-null    int32   \n",
      " 18  Title_Code     418 non-null    int32   \n",
      " 19  AgeBin_Code    418 non-null    int32   \n",
      " 20  FareBin_Code   418 non-null    int32   \n",
      "dtypes: category(2), float64(2), int32(5), int64(6), object(6)\n",
      "memory usage: 55.3+ KB\n",
      "None\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>204</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>891</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Karun, Miss. Manca</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B96 B98</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PassengerId    Survived      Pclass                Name   Sex  \\\n",
       "count    891.000000  891.000000  891.000000                 891   891   \n",
       "unique          NaN         NaN         NaN                 891     2   \n",
       "top             NaN         NaN         NaN  Karun, Miss. Manca  male   \n",
       "freq            NaN         NaN         NaN                   1   577   \n",
       "mean     446.000000    0.383838    2.308642                 NaN   NaN   \n",
       "std      257.353842    0.486592    0.836071                 NaN   NaN   \n",
       "min        1.000000    0.000000    1.000000                 NaN   NaN   \n",
       "25%      223.500000    0.000000    2.000000                 NaN   NaN   \n",
       "50%      446.000000    0.000000    3.000000                 NaN   NaN   \n",
       "75%      668.500000    1.000000    3.000000                 NaN   NaN   \n",
       "max      891.000000    1.000000    3.000000                 NaN   NaN   \n",
       "\n",
       "               Age       SibSp       Parch Ticket        Fare    Cabin  \\\n",
       "count   714.000000  891.000000  891.000000    891  891.000000      204   \n",
       "unique         NaN         NaN         NaN    681         NaN      147   \n",
       "top            NaN         NaN         NaN   1601         NaN  B96 B98   \n",
       "freq           NaN         NaN         NaN      7         NaN        4   \n",
       "mean     29.699118    0.523008    0.381594    NaN   32.204208      NaN   \n",
       "std      14.526497    1.102743    0.806057    NaN   49.693429      NaN   \n",
       "min       0.420000    0.000000    0.000000    NaN    0.000000      NaN   \n",
       "25%      20.125000    0.000000    0.000000    NaN    7.910400      NaN   \n",
       "50%      28.000000    0.000000    0.000000    NaN   14.454200      NaN   \n",
       "75%      38.000000    1.000000    0.000000    NaN   31.000000      NaN   \n",
       "max      80.000000    8.000000    6.000000    NaN  512.329200      NaN   \n",
       "\n",
       "       Embarked  \n",
       "count       889  \n",
       "unique        3  \n",
       "top           S  \n",
       "freq        644  \n",
       "mean        NaN  \n",
       "std         NaN  \n",
       "min         NaN  \n",
       "25%         NaN  \n",
       "50%         NaN  \n",
       "75%         NaN  \n",
       "max         NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train columns with null values: \\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data1.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values: \\n', data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data_val.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "data_raw.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data1 Shape: (891, 19)\n",
      "Train1 Shape: (668, 8)\n",
      "Test1 Shape: (223, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex_Code</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Embarked_Code</th>\n",
       "      <th>Title_Code</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>AgeBin_Code</th>\n",
       "      <th>FareBin_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sex_Code  Pclass  Embarked_Code  Title_Code  FamilySize  AgeBin_Code  \\\n",
       "105         1       3              2           3           1            1   \n",
       "68          0       3              2           2           7            1   \n",
       "253         1       3              2           3           2            1   \n",
       "320         1       3              2           3           1            1   \n",
       "706         0       2              2           4           1            2   \n",
       "\n",
       "     FareBin_Code  \n",
       "105             0  \n",
       "68              1  \n",
       "253             2  \n",
       "320             0  \n",
       "706             1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split train and test data with function defaults\n",
    "#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n",
    "train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\n",
    "train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\n",
    "train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n",
    "\n",
    "\n",
    "print(\"Data1 Shape: {}\".format(data1.shape))\n",
    "print(\"Train1 Shape: {}\".format(train1_x.shape))\n",
    "print(\"Test1 Shape: {}\".format(test1_x.shape))\n",
    "\n",
    "train1_x_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_x_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bc5f0a0b08>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n",
    "#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n",
    "#and subplotS: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html?highlight=matplotlib%20pyplot%20subplots#matplotlib.pyplot.subplots\n",
    "\n",
    "#graph distribution of quantitative data\n",
    "plt.figure(figsize=[16,12])\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.boxplot(x=data1['Fare'], showmeans = True, meanline = True)\n",
    "plt.title('Fare Boxplot')\n",
    "plt.ylabel('Fare ($)')\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.boxplot(data1['Age'], showmeans = True, meanline = True)\n",
    "plt.title('Age Boxplot')\n",
    "plt.ylabel('Age (Years)')\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.boxplot(data1['FamilySize'], showmeans = True, meanline = True)\n",
    "plt.title('Family Size Boxplot')\n",
    "plt.ylabel('Family Size (#)')\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.hist(x = [data1[data1['Survived']==1]['Fare'], data1[data1['Survived']==0]['Fare']], \n",
    "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
    "plt.title('Fare Histogram by Survival')\n",
    "plt.xlabel('Fare ($)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.hist(x = [data1[data1['Survived']==1]['Age'], data1[data1['Survived']==0]['Age']], \n",
    "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
    "plt.title('Age Histogram by Survival')\n",
    "plt.xlabel('Age (Years)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.hist(x = [data1[data1['Survived']==1]['FamilySize'], data1[data1['Survived']==0]['FamilySize']], \n",
    "         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\n",
    "plt.title('Family Size Histogram by Survival')\n",
    "plt.xlabel('Family Size (#)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_nn_model():\n",
    "\n",
    "#     classifier = Sequential()\n",
    "#     classifier.add(Dense(activation=\"relu\", input_dim=7, units=11, kernel_initializer=\"uniform\"))\n",
    "#     classifier.add(Dense(activation=\"relu\", units=11, kernel_initializer=\"uniform\"))\n",
    "#     classifier.add(Dropout(0.5))\n",
    "#     classifier.add(Dense(activation=\"relu\", units=11, kernel_initializer=\"uniform\"))\n",
    "#     classifier.add(Dropout(0.5))\n",
    "#     classifier.add(Dense(activation=\"relu\", units=5, kernel_initializer=\"uniform\"))\n",
    "#     classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n",
    "#     classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "#     return classifier\n",
    "    model = Sequential()\n",
    "    model.add(Dense(activation=\"relu\", input_dim=17, units=11, kernel_initializer=\"uniform\"))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "#     model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "MLA = [\n",
    "    # i. Ensemble Methods\n",
    "    ensemble.RandomForestClassifier(),\n",
    "    \n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.AdaBoostClassifier( tree.DecisionTreeClassifier(random_state = 11, max_features = \"auto\",max_depth = None)),\n",
    "    \n",
    "    \n",
    "    # ii. SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "\n",
    "    # iii. Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "     XGBClassifier()   \n",
    "#         # v. PCA \n",
    "#     cluster.KMeans(n_clusters=2),\n",
    "#     # iv. Neutal Nets \n",
    "#     _get_nn_model() \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived\n",
       "0           0\n",
       "1           1\n",
       "2           1\n",
       "3           1\n",
       "4           0\n",
       "..        ...\n",
       "886         0\n",
       "887         1\n",
       "888         0\n",
       "889         1\n",
       "890         0\n",
       "\n",
       "[891 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[Target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLA Name</th>\n",
       "      <th>MLA Parameters</th>\n",
       "      <th>MLA Train Accuracy Mean</th>\n",
       "      <th>MLA Test Accuracy Mean</th>\n",
       "      <th>MLA Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...</td>\n",
       "      <td>0.893071</td>\n",
       "      <td>0.828358</td>\n",
       "      <td>0.121573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NuSVC</td>\n",
       "      <td>{'break_ties': False, 'cache_size': 200, 'clas...</td>\n",
       "      <td>0.834644</td>\n",
       "      <td>0.826866</td>\n",
       "      <td>0.0294275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC</td>\n",
       "      <td>{'C': 1.0, 'break_ties': False, 'cache_size': ...</td>\n",
       "      <td>0.835581</td>\n",
       "      <td>0.826493</td>\n",
       "      <td>0.0258159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>{'objective': 'binary:logistic', 'base_score':...</td>\n",
       "      <td>0.88839</td>\n",
       "      <td>0.826493</td>\n",
       "      <td>0.0282238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BaggingClassifier</td>\n",
       "      <td>{'base_estimator': None, 'bootstrap': True, 'b...</td>\n",
       "      <td>0.890075</td>\n",
       "      <td>0.820149</td>\n",
       "      <td>0.0175043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator__ccp_...</td>\n",
       "      <td>0.893071</td>\n",
       "      <td>0.81306</td>\n",
       "      <td>0.060138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>{'algorithm': 'auto', 'leaf_size': 30, 'metric...</td>\n",
       "      <td>0.847004</td>\n",
       "      <td>0.804104</td>\n",
       "      <td>0.00379219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': True,...</td>\n",
       "      <td>0.801124</td>\n",
       "      <td>0.796269</td>\n",
       "      <td>0.0157592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MLA Name                                     MLA Parameters  \\\n",
       "0  RandomForestClassifier  {'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...   \n",
       "4                   NuSVC  {'break_ties': False, 'cache_size': 200, 'clas...   \n",
       "3                     SVC  {'C': 1.0, 'break_ties': False, 'cache_size': ...   \n",
       "7           XGBClassifier  {'objective': 'binary:logistic', 'base_score':...   \n",
       "1       BaggingClassifier  {'base_estimator': None, 'bootstrap': True, 'b...   \n",
       "2      AdaBoostClassifier  {'algorithm': 'SAMME.R', 'base_estimator__ccp_...   \n",
       "6    KNeighborsClassifier  {'algorithm': 'auto', 'leaf_size': 30, 'metric...   \n",
       "5               LinearSVC  {'C': 1.0, 'class_weight': None, 'dual': True,...   \n",
       "\n",
       "  MLA Train Accuracy Mean MLA Test Accuracy Mean    MLA Time  \n",
       "0                0.893071               0.828358    0.121573  \n",
       "4                0.834644               0.826866   0.0294275  \n",
       "3                0.835581               0.826493   0.0258159  \n",
       "7                 0.88839               0.826493   0.0282238  \n",
       "1                0.890075               0.820149   0.0175043  \n",
       "2                0.893071                0.81306    0.060138  \n",
       "6                0.847004               0.804104  0.00379219  \n",
       "5                0.801124               0.796269   0.0157592  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean' ,'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "MLA_predict = data1[Target]\n",
    "MLA_predict_test =  pd.DataFrame()\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "    \n",
    "    \n",
    "#     #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "#     print(MLA_name)\n",
    "#     if MLA_name == 'Sequential':\n",
    "#         MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "# #         MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    " \n",
    "# #         model = KerasClassifier(build_fn=_get_nn_model, epochs=100, batch_size=10)\n",
    "# #         kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "# #         cv_results = model_selection.cross_validate(model, data1[data1_x_bin], data1[Target], cv=kfold,return_train_score=True)\n",
    "# # #         cv_results = model_selection.cross_validate(model , data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True)\n",
    "# #         MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "# #         MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "# #         MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()  \n",
    "               \n",
    "#         history = alg.fit(data1[data1_x_bin], data1[Target], batch_size = 10, epochs = 300,validation_split=0.1,verbose = 1,shuffle=True)\n",
    "#         MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n",
    "#         print(alg.predict_classes(data_val[data1_x_bin] ))\n",
    "#         df = alg.predict_classes(data_val[data1_x_bin] )\n",
    "#         row_index+=1\n",
    "#         #         MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "# #         MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "# #         MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()  \n",
    "#         continue\n",
    "   \n",
    " \n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True)\n",
    "\n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "    \n",
    "\n",
    "#     #save MLA predictions - see section 6 for usage\n",
    "    alg.fit(data1[data1_x_bin], data1[Target])\n",
    "    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n",
    "    row_index+=1\n",
    "\n",
    "    \n",
    "\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "MLA_compare\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(668, 17)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split train and test data with function defaults\n",
    "#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n",
    "# train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\n",
    "# train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\n",
    "# train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n",
    "train1_x_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLA_compare.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(activation=\"relu\", input_dim=17, units=11, kernel_initializer=\"uniform\"))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "         loss='binary_crossentropy',\n",
    "         metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 2/61 [..............................] - ETA: 6s - loss: 0.1840 - accuracy: 0.9500WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.115691). Check your callbacks.\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 0.4551 - accuracy: 0.7987 - val_loss: 0.5894 - val_accuracy: 0.7313\n",
      "Epoch 2/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4618 - accuracy: 0.7903 - val_loss: 0.5407 - val_accuracy: 0.7910\n",
      "Epoch 3/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4505 - accuracy: 0.8120 - val_loss: 0.4823 - val_accuracy: 0.7910\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4372 - accuracy: 0.8053 - val_loss: 0.4999 - val_accuracy: 0.7910\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4372 - accuracy: 0.8020 - val_loss: 0.4777 - val_accuracy: 0.8209\n",
      "Epoch 6/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4364 - accuracy: 0.8186 - val_loss: 0.4697 - val_accuracy: 0.8060\n",
      "Epoch 7/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.8103 - val_loss: 0.4703 - val_accuracy: 0.8060\n",
      "Epoch 8/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4398 - accuracy: 0.8153 - val_loss: 0.4788 - val_accuracy: 0.8060\n",
      "Epoch 9/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4281 - accuracy: 0.8120 - val_loss: 0.4876 - val_accuracy: 0.8209\n",
      "Epoch 10/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4213 - accuracy: 0.8253 - val_loss: 0.4701 - val_accuracy: 0.8209\n",
      "Epoch 11/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4340 - accuracy: 0.8170 - val_loss: 0.4606 - val_accuracy: 0.8358\n",
      "Epoch 12/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4222 - accuracy: 0.8253 - val_loss: 0.4580 - val_accuracy: 0.8358\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4207 - accuracy: 0.8303 - val_loss: 0.4730 - val_accuracy: 0.8209\n",
      "Epoch 14/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4204 - accuracy: 0.8186 - val_loss: 0.4427 - val_accuracy: 0.8209\n",
      "Epoch 15/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4267 - accuracy: 0.8236 - val_loss: 0.4432 - val_accuracy: 0.8209\n",
      "Epoch 16/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4166 - accuracy: 0.8236 - val_loss: 0.4742 - val_accuracy: 0.8358\n",
      "Epoch 17/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4313 - accuracy: 0.8253 - val_loss: 0.4448 - val_accuracy: 0.8060\n",
      "Epoch 18/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4121 - accuracy: 0.8336 - val_loss: 0.4535 - val_accuracy: 0.8060\n",
      "Epoch 19/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4089 - accuracy: 0.8369 - val_loss: 0.4599 - val_accuracy: 0.8209\n",
      "Epoch 20/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4119 - accuracy: 0.8203 - val_loss: 0.4385 - val_accuracy: 0.8358\n",
      "Epoch 21/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4039 - accuracy: 0.8270 - val_loss: 0.4366 - val_accuracy: 0.8358\n",
      "Epoch 22/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4090 - accuracy: 0.8270 - val_loss: 0.4476 - val_accuracy: 0.8507\n",
      "Epoch 23/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4083 - accuracy: 0.8303 - val_loss: 0.4640 - val_accuracy: 0.8358\n",
      "Epoch 24/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4032 - accuracy: 0.8253 - val_loss: 0.4926 - val_accuracy: 0.8358\n",
      "Epoch 25/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4036 - accuracy: 0.8353 - val_loss: 0.4436 - val_accuracy: 0.8358\n",
      "Epoch 26/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4001 - accuracy: 0.8369 - val_loss: 0.4382 - val_accuracy: 0.8507\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3986 - accuracy: 0.8353 - val_loss: 0.4383 - val_accuracy: 0.8507\n",
      "Epoch 28/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3975 - accuracy: 0.8386 - val_loss: 0.4350 - val_accuracy: 0.8060\n",
      "Epoch 29/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4045 - accuracy: 0.8336 - val_loss: 0.4422 - val_accuracy: 0.8209\n",
      "Epoch 30/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4091 - accuracy: 0.8303 - val_loss: 0.4620 - val_accuracy: 0.8507\n",
      "Epoch 31/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4075 - accuracy: 0.8270 - val_loss: 0.4469 - val_accuracy: 0.8060\n",
      "Epoch 32/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4010 - accuracy: 0.8270 - val_loss: 0.4361 - val_accuracy: 0.8507\n",
      "Epoch 33/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3924 - accuracy: 0.8369 - val_loss: 0.4314 - val_accuracy: 0.8358\n",
      "Epoch 34/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8336 - val_loss: 0.4334 - val_accuracy: 0.8060\n",
      "Epoch 35/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3985 - accuracy: 0.8369 - val_loss: 0.4382 - val_accuracy: 0.8358\n",
      "Epoch 36/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3899 - accuracy: 0.8353 - val_loss: 0.4796 - val_accuracy: 0.8209\n",
      "Epoch 37/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3864 - accuracy: 0.8403 - val_loss: 0.4391 - val_accuracy: 0.8060\n",
      "Epoch 38/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3890 - accuracy: 0.8336 - val_loss: 0.5088 - val_accuracy: 0.8209\n",
      "Epoch 39/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4043 - accuracy: 0.8286 - val_loss: 0.4566 - val_accuracy: 0.8209\n",
      "Epoch 40/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3917 - accuracy: 0.8403 - val_loss: 0.4495 - val_accuracy: 0.8507\n",
      "Epoch 41/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4015 - accuracy: 0.8319 - val_loss: 0.4884 - val_accuracy: 0.8209\n",
      "Epoch 42/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3966 - accuracy: 0.8319 - val_loss: 0.4435 - val_accuracy: 0.8507\n",
      "Epoch 43/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3832 - accuracy: 0.8286 - val_loss: 0.4453 - val_accuracy: 0.8358\n",
      "Epoch 44/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3876 - accuracy: 0.8436 - val_loss: 0.4394 - val_accuracy: 0.8507\n",
      "Epoch 45/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3836 - accuracy: 0.8353 - val_loss: 0.4651 - val_accuracy: 0.8209\n",
      "Epoch 46/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3870 - accuracy: 0.8353 - val_loss: 0.4511 - val_accuracy: 0.8358\n",
      "Epoch 47/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3829 - accuracy: 0.8403 - val_loss: 0.4540 - val_accuracy: 0.8657\n",
      "Epoch 48/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3759 - accuracy: 0.8353 - val_loss: 0.4577 - val_accuracy: 0.8507\n",
      "Epoch 49/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3796 - accuracy: 0.8386 - val_loss: 0.4470 - val_accuracy: 0.8657\n",
      "Epoch 50/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3877 - accuracy: 0.8270 - val_loss: 0.4875 - val_accuracy: 0.8060\n",
      "Epoch 51/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3978 - accuracy: 0.8353 - val_loss: 0.4465 - val_accuracy: 0.8358\n",
      "Epoch 52/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3790 - accuracy: 0.8403 - val_loss: 0.4910 - val_accuracy: 0.8209\n",
      "Epoch 53/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3741 - accuracy: 0.8386 - val_loss: 0.4606 - val_accuracy: 0.8209\n",
      "Epoch 54/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4233 - accuracy: 0.8286 - val_loss: 0.4597 - val_accuracy: 0.8209\n",
      "Epoch 55/100\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 0.3918 - accuracy: 0.8336 - val_loss: 0.4469 - val_accuracy: 0.8209\n",
      "Epoch 56/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3792 - accuracy: 0.8436 - val_loss: 0.4472 - val_accuracy: 0.8507\n",
      "Epoch 57/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3808 - accuracy: 0.8436 - val_loss: 0.4505 - val_accuracy: 0.8657\n",
      "Epoch 58/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3743 - accuracy: 0.8353 - val_loss: 0.4689 - val_accuracy: 0.8209\n",
      "Epoch 59/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.4011 - accuracy: 0.8203 - val_loss: 0.4731 - val_accuracy: 0.8209\n",
      "Epoch 60/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3747 - accuracy: 0.8386 - val_loss: 0.4503 - val_accuracy: 0.8358\n",
      "Epoch 61/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3791 - accuracy: 0.8386 - val_loss: 0.4768 - val_accuracy: 0.8209\n",
      "Epoch 62/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3702 - accuracy: 0.8386 - val_loss: 0.4550 - val_accuracy: 0.8358\n",
      "Epoch 63/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3689 - accuracy: 0.8453 - val_loss: 0.4438 - val_accuracy: 0.8209\n",
      "Epoch 64/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3758 - accuracy: 0.8386 - val_loss: 0.4610 - val_accuracy: 0.8209\n",
      "Epoch 65/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3806 - accuracy: 0.8403 - val_loss: 0.5012 - val_accuracy: 0.7910\n",
      "Epoch 66/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3735 - accuracy: 0.8369 - val_loss: 0.4768 - val_accuracy: 0.8209\n",
      "Epoch 67/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3754 - accuracy: 0.8386 - val_loss: 0.4524 - val_accuracy: 0.8358\n",
      "Epoch 68/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3707 - accuracy: 0.8386 - val_loss: 0.4741 - val_accuracy: 0.8209\n",
      "Epoch 69/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3725 - accuracy: 0.8403 - val_loss: 0.4849 - val_accuracy: 0.8209\n",
      "Epoch 70/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3716 - accuracy: 0.8403 - val_loss: 0.4762 - val_accuracy: 0.8209\n",
      "Epoch 71/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3705 - accuracy: 0.8419 - val_loss: 0.4603 - val_accuracy: 0.8209\n",
      "Epoch 72/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3718 - accuracy: 0.8502 - val_loss: 0.4909 - val_accuracy: 0.8209\n",
      "Epoch 73/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3681 - accuracy: 0.8453 - val_loss: 0.5249 - val_accuracy: 0.8209\n",
      "Epoch 74/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3726 - accuracy: 0.8386 - val_loss: 0.4970 - val_accuracy: 0.8209\n",
      "Epoch 75/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3654 - accuracy: 0.8453 - val_loss: 0.4555 - val_accuracy: 0.8209\n",
      "Epoch 76/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3659 - accuracy: 0.8403 - val_loss: 0.4698 - val_accuracy: 0.8209\n",
      "Epoch 77/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3727 - accuracy: 0.8403 - val_loss: 0.4587 - val_accuracy: 0.8209\n",
      "Epoch 78/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3672 - accuracy: 0.8369 - val_loss: 0.4916 - val_accuracy: 0.8209\n",
      "Epoch 79/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3633 - accuracy: 0.8419 - val_loss: 0.4478 - val_accuracy: 0.8358\n",
      "Epoch 80/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3863 - accuracy: 0.8319 - val_loss: 0.4598 - val_accuracy: 0.8209\n",
      "Epoch 81/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3647 - accuracy: 0.8403 - val_loss: 0.4832 - val_accuracy: 0.8060\n",
      "Epoch 82/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3663 - accuracy: 0.8436 - val_loss: 0.4665 - val_accuracy: 0.8209\n",
      "Epoch 83/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3762 - accuracy: 0.8319 - val_loss: 0.6424 - val_accuracy: 0.6716\n",
      "Epoch 84/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3870 - accuracy: 0.8336 - val_loss: 0.4740 - val_accuracy: 0.8209\n",
      "Epoch 85/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3634 - accuracy: 0.8453 - val_loss: 0.4934 - val_accuracy: 0.8209\n",
      "Epoch 86/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3606 - accuracy: 0.8419 - val_loss: 0.4825 - val_accuracy: 0.8209\n",
      "Epoch 87/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3658 - accuracy: 0.8502 - val_loss: 0.4885 - val_accuracy: 0.8209\n",
      "Epoch 88/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3665 - accuracy: 0.8519 - val_loss: 0.4488 - val_accuracy: 0.8209\n",
      "Epoch 89/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.8486 - val_loss: 0.4619 - val_accuracy: 0.8209\n",
      "Epoch 90/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.8403 - val_loss: 0.4501 - val_accuracy: 0.8507\n",
      "Epoch 91/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3702 - accuracy: 0.8419 - val_loss: 0.4952 - val_accuracy: 0.8209\n",
      "Epoch 92/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3769 - accuracy: 0.8353 - val_loss: 0.4694 - val_accuracy: 0.8209\n",
      "Epoch 93/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3646 - accuracy: 0.8486 - val_loss: 0.4684 - val_accuracy: 0.8358\n",
      "Epoch 94/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3637 - accuracy: 0.8419 - val_loss: 0.4662 - val_accuracy: 0.8358\n",
      "Epoch 95/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3627 - accuracy: 0.8453 - val_loss: 0.4930 - val_accuracy: 0.8358\n",
      "Epoch 96/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3643 - accuracy: 0.8436 - val_loss: 0.5239 - val_accuracy: 0.7612\n",
      "Epoch 97/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3688 - accuracy: 0.8386 - val_loss: 0.4526 - val_accuracy: 0.8358\n",
      "Epoch 98/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3646 - accuracy: 0.8336 - val_loss: 0.4474 - val_accuracy: 0.8507\n",
      "Epoch 99/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3672 - accuracy: 0.8486 - val_loss: 0.4979 - val_accuracy: 0.8209\n",
      "Epoch 100/100\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.3642 - accuracy: 0.8419 - val_loss: 0.4886 - val_accuracy: 0.8209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ab77a23d08>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "model.fit(train1_x_dummy, train1_y_dummy, batch_size = 10, epochs = 100,validation_split=0.1,verbose = 1,shuffle=True\n",
    "#           ,callbacks=[tensorboard_callback]\n",
    "         )\n",
    "# y_pred=model.predict_classes(test1_x_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8116591928251121\n"
     ]
    }
   ],
   "source": [
    "y_pred=(model.predict(test1_x_dummy) > 0.5).astype(\"int32\")\n",
    "print(metrics.accuracy_score(test1_y_dummy, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4140 - accuracy: 0.8117\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test1_x_dummy, test1_y_dummy, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4320 candidates, totalling 12960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    4.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-213-5541ce51cc39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m                       n_jobs = 2)\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mbestF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgridF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain1_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain1_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbestF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest1_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest1_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    713\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 715\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    538\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    539\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    541\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tf-gpu\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tf-gpu\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#optimization\n",
    "\n",
    "\n",
    "\n",
    "#RandomForestClassifier\n",
    "\n",
    "MLA[0]\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 200, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 60, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "gridF = model_selection.GridSearchCV(MLA[0], random_grid, cv = 3, verbose = 1, \n",
    "                      n_jobs = 2)\n",
    "\n",
    "bestF = gridF.fit(train1_x, train1_y)\n",
    "y_pred=bestF.predict(test1_x)\n",
    "print(metrics.accuracy_score(test1_y, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLA[0]= bestF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 45,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 4,\n",
       " 'min_samples_split': 10,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestF.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier()"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLA[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8116591928251121\n"
     ]
    }
   ],
   "source": [
    "#BaggingClassifier\n",
    "MLA[1]\n",
    "\n",
    "param_grid = {\n",
    "    'base_estimator__max_depth' : [1, 2, 3, 4, 5],\n",
    "    'max_samples' : [0.05, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "gridF = model_selection.GridSearchCV(ensemble.BaggingClassifier(tree.DecisionTreeClassifier(),\n",
    "                                     n_estimators = 100, max_features = 0.5),\n",
    "                   param_grid)\n",
    "\n",
    "bestB = gridF.fit(train1_x, train1_y)\n",
    "y_pred=bestB.predict(test1_x)\n",
    "print(metrics.accuracy_score(test1_y, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator__max_depth': 5, 'max_samples': 0.5}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLA[1] = bestB\n",
    "bestB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8026905829596412\n"
     ]
    }
   ],
   "source": [
    "# AdaBoostClassifier\n",
    "MLA[2]\n",
    "# param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "# #               \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "#               \"n_estimators\": [1, 2]\n",
    "#              }\n",
    "\n",
    "# gridF = model_selection.GridSearchCV(MLA[2], param_grid, cv = 3, verbose = 1, \n",
    "#                       n_jobs = -1)\n",
    "\n",
    "# bestADA = gridF.fit(train1_x, train1_y)\n",
    "# y_pred=bestADA.predict(test1_x)\n",
    "# print(metrics.accuracy_score(test1_y, y_pred))\n",
    "\n",
    "param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"n_estimators\": [100,200,250,300]\n",
    "             }\n",
    "\n",
    "\n",
    "DTC = tree.DecisionTreeClassifier(random_state = 11, max_features = \"auto\",max_depth = None)\n",
    "\n",
    "ABC = ensemble.AdaBoostClassifier(base_estimator = DTC)\n",
    "\n",
    "# run grid search\n",
    "grid_search_ABC = model_selection.GridSearchCV(MLA[2], param_grid=param_grid, scoring = 'roc_auc')\n",
    "bestADA = grid_search_ABC.fit(train1_x, train1_y)\n",
    "y_pred=bestADA.predict(test1_x)\n",
    "print(metrics.accuracy_score(test1_y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator__criterion': 'entropy',\n",
       " 'base_estimator__splitter': 'random',\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLA[2] = bestADA\n",
    "bestADA.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8026905829596412\n"
     ]
    }
   ],
   "source": [
    "# SVC\n",
    "\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "\n",
    "\n",
    "grid_search_SVC =  model_selection.GridSearchCV(\n",
    "        MLA[3] , param_grid)\n",
    "bestSVC = grid_search_SVC.fit(train1_x, train1_y)  \n",
    "y_pred=bestADA.predict(test1_x)\n",
    "print(metrics.accuracy_score(test1_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.01}"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLA[3] = bestSVC\n",
    "bestSVC.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.757847533632287\n"
     ]
    }
   ],
   "source": [
    "# NuSVC\n",
    "MLA[4]\n",
    "\n",
    "# LinearSVC\n",
    "MLA[5]\n",
    "\n",
    "# KNeighborsClassifier\n",
    "MLA[6]\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "\n",
    "#Convert to dictionary\n",
    "param_grid_knn = {'leaf_size':leaf_size, 'n_neighbors':n_neighbors, 'p':p}\n",
    "grid_search_SVC =  model_selection.GridSearchCV(\n",
    "        MLA[6] , param_grid_knn)\n",
    "bestKNN = grid_search_SVC.fit(train1_x, train1_y)  \n",
    "y_pred=bestKNN.predict(test1_x)\n",
    "print(metrics.accuracy_score(test1_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'leaf_size': 2, 'n_neighbors': 17, 'p': 1}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLA[6] = bestKNN \n",
    "bestKNN.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8295964125560538\n"
     ]
    }
   ],
   "source": [
    "MLA[7]\n",
    "\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "grid_search_SVC =  model_selection.GridSearchCV(\n",
    "        MLA[7] , params)\n",
    "bestXGB = grid_search_SVC.fit(train1_x, train1_y)  \n",
    "y_pred=bestXGB.predict(test1_x)\n",
    "print(metrics.accuracy_score(test1_y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 1.0,\n",
       " 'gamma': 5,\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 1,\n",
       " 'subsample': 0.6}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLA[7] = bestXGB \n",
    "bestXGB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6783916354179382,\n",
       "  0.6289079785346985,\n",
       "  0.5586045384407043,\n",
       "  0.4880298376083374,\n",
       "  0.4669020175933838,\n",
       "  0.46355459094047546,\n",
       "  0.44893765449523926,\n",
       "  0.4438673257827759,\n",
       "  0.43639612197875977,\n",
       "  0.43823349475860596,\n",
       "  0.44126901030540466,\n",
       "  0.43248435854911804,\n",
       "  0.43092766404151917,\n",
       "  0.42577439546585083,\n",
       "  0.4297657608985901,\n",
       "  0.42571839690208435,\n",
       "  0.42931675910949707,\n",
       "  0.4257147014141083,\n",
       "  0.41875341534614563,\n",
       "  0.42348384857177734,\n",
       "  0.4252137839794159,\n",
       "  0.42013704776763916,\n",
       "  0.4240877330303192,\n",
       "  0.4144645631313324,\n",
       "  0.414436399936676,\n",
       "  0.4131908118724823,\n",
       "  0.41264140605926514,\n",
       "  0.4119069278240204,\n",
       "  0.4168799817562103,\n",
       "  0.41719117760658264,\n",
       "  0.409798264503479,\n",
       "  0.40736234188079834,\n",
       "  0.4125697612762451,\n",
       "  0.4201909005641937,\n",
       "  0.40546178817749023,\n",
       "  0.40878114104270935,\n",
       "  0.41132852435112,\n",
       "  0.4054712951183319,\n",
       "  0.4076751470565796,\n",
       "  0.4073229432106018,\n",
       "  0.4081881046295166,\n",
       "  0.4099882245063782,\n",
       "  0.4027503430843353,\n",
       "  0.4004053771495819,\n",
       "  0.401856392621994,\n",
       "  0.40469637513160706,\n",
       "  0.40546858310699463,\n",
       "  0.403318852186203,\n",
       "  0.39794406294822693,\n",
       "  0.40136513113975525,\n",
       "  0.39892929792404175,\n",
       "  0.40358686447143555,\n",
       "  0.39991480112075806,\n",
       "  0.3972361087799072,\n",
       "  0.3969646096229553,\n",
       "  0.40521708130836487,\n",
       "  0.39771074056625366,\n",
       "  0.396106094121933,\n",
       "  0.39737504720687866,\n",
       "  0.39904671907424927,\n",
       "  0.3989100158214569,\n",
       "  0.399951696395874,\n",
       "  0.3984139859676361,\n",
       "  0.40893614292144775,\n",
       "  0.3995122015476227,\n",
       "  0.3950214982032776,\n",
       "  0.3924051821231842,\n",
       "  0.3990093767642975,\n",
       "  0.3985556960105896,\n",
       "  0.3974371552467346,\n",
       "  0.4020250737667084,\n",
       "  0.4038151800632477,\n",
       "  0.39843252301216125,\n",
       "  0.39703038334846497,\n",
       "  0.39767372608184814,\n",
       "  0.39437857270240784,\n",
       "  0.39103707671165466,\n",
       "  0.40091314911842346,\n",
       "  0.39278069138526917,\n",
       "  0.39746323227882385,\n",
       "  0.39348164200782776,\n",
       "  0.3979881703853607,\n",
       "  0.39508700370788574,\n",
       "  0.3894106447696686,\n",
       "  0.3930513262748718,\n",
       "  0.3904324471950531,\n",
       "  0.39136484265327454,\n",
       "  0.392261266708374,\n",
       "  0.4037264883518219,\n",
       "  0.39643335342407227,\n",
       "  0.3911994993686676,\n",
       "  0.3948371112346649,\n",
       "  0.3948621451854706,\n",
       "  0.3937748670578003,\n",
       "  0.39203882217407227,\n",
       "  0.3889291286468506,\n",
       "  0.3877762258052826,\n",
       "  0.3893708884716034,\n",
       "  0.3965509831905365,\n",
       "  0.3970828056335449,\n",
       "  0.3962780237197876,\n",
       "  0.3909060060977936,\n",
       "  0.3909141421318054,\n",
       "  0.39288365840911865,\n",
       "  0.39265745878219604,\n",
       "  0.3912913501262665,\n",
       "  0.3882123529911041,\n",
       "  0.3996793031692505,\n",
       "  0.3907027542591095,\n",
       "  0.38882049918174744,\n",
       "  0.39060908555984497,\n",
       "  0.39033856987953186,\n",
       "  0.38686513900756836,\n",
       "  0.3917228579521179,\n",
       "  0.38713669776916504,\n",
       "  0.3849945366382599,\n",
       "  0.38907188177108765,\n",
       "  0.3869190812110901,\n",
       "  0.3913506865501404,\n",
       "  0.38901519775390625,\n",
       "  0.3854682147502899,\n",
       "  0.38879042863845825,\n",
       "  0.39512842893600464,\n",
       "  0.3853868842124939,\n",
       "  0.3863910436630249,\n",
       "  0.38697558641433716,\n",
       "  0.392098069190979,\n",
       "  0.3859931230545044,\n",
       "  0.3832027018070221,\n",
       "  0.39699509739875793,\n",
       "  0.39616814255714417,\n",
       "  0.38650277256965637,\n",
       "  0.38813063502311707,\n",
       "  0.386630654335022,\n",
       "  0.38874977827072144,\n",
       "  0.39374521374702454,\n",
       "  0.38449111580848694,\n",
       "  0.39055466651916504,\n",
       "  0.38709086179733276,\n",
       "  0.38865527510643005,\n",
       "  0.38310304284095764,\n",
       "  0.3904888331890106,\n",
       "  0.38373270630836487,\n",
       "  0.3785429298877716,\n",
       "  0.3791695237159729,\n",
       "  0.38511428236961365,\n",
       "  0.3834914565086365,\n",
       "  0.39185065031051636,\n",
       "  0.39085307717323303,\n",
       "  0.3862488269805908,\n",
       "  0.38667032122612,\n",
       "  0.3847759962081909,\n",
       "  0.3889581561088562,\n",
       "  0.38684606552124023,\n",
       "  0.39098843932151794,\n",
       "  0.3844758868217468,\n",
       "  0.38033589720726013,\n",
       "  0.38367506861686707,\n",
       "  0.3868032395839691,\n",
       "  0.3902216851711273,\n",
       "  0.380045622587204,\n",
       "  0.38694658875465393,\n",
       "  0.38128936290740967,\n",
       "  0.38168925046920776,\n",
       "  0.3797588348388672,\n",
       "  0.38264182209968567,\n",
       "  0.37612321972846985,\n",
       "  0.3853663504123688,\n",
       "  0.39517974853515625,\n",
       "  0.3822721242904663,\n",
       "  0.38121530413627625,\n",
       "  0.38098570704460144,\n",
       "  0.38742658495903015,\n",
       "  0.38287654519081116,\n",
       "  0.37991541624069214,\n",
       "  0.3835589289665222,\n",
       "  0.38080188632011414,\n",
       "  0.3793049454689026,\n",
       "  0.37892743945121765,\n",
       "  0.3820950984954834,\n",
       "  0.38141128420829773,\n",
       "  0.37465420365333557,\n",
       "  0.3761211931705475,\n",
       "  0.3799362778663635,\n",
       "  0.38031917810440063,\n",
       "  0.3814440071582794,\n",
       "  0.3859485685825348,\n",
       "  0.382101446390152,\n",
       "  0.3773996829986572,\n",
       "  0.37792882323265076,\n",
       "  0.3744535446166992,\n",
       "  0.3761971592903137,\n",
       "  0.3832230567932129,\n",
       "  0.377515971660614,\n",
       "  0.3772013187408447,\n",
       "  0.3925877511501312,\n",
       "  0.38114646077156067,\n",
       "  0.38236749172210693,\n",
       "  0.37543103098869324,\n",
       "  0.37445709109306335,\n",
       "  0.37344226241111755,\n",
       "  0.37716713547706604,\n",
       "  0.3758179545402527,\n",
       "  0.378532737493515,\n",
       "  0.3751554489135742,\n",
       "  0.3744836747646332,\n",
       "  0.37465885281562805,\n",
       "  0.3794439733028412,\n",
       "  0.3730049729347229,\n",
       "  0.3728213310241699,\n",
       "  0.3788229525089264,\n",
       "  0.37367114424705505,\n",
       "  0.38031747937202454,\n",
       "  0.3760300576686859,\n",
       "  0.3721439838409424,\n",
       "  0.3733750283718109,\n",
       "  0.36962684988975525,\n",
       "  0.3778782784938812,\n",
       "  0.37885844707489014,\n",
       "  0.37627145648002625,\n",
       "  0.371551513671875,\n",
       "  0.3696017265319824,\n",
       "  0.37236711382865906,\n",
       "  0.37076863646507263,\n",
       "  0.3716529607772827,\n",
       "  0.3702056109905243,\n",
       "  0.37089240550994873,\n",
       "  0.3751416504383087,\n",
       "  0.3743654489517212,\n",
       "  0.3748261034488678,\n",
       "  0.37162870168685913,\n",
       "  0.3682592809200287,\n",
       "  0.3691718280315399,\n",
       "  0.3692384660243988,\n",
       "  0.3700648844242096,\n",
       "  0.3707965612411499,\n",
       "  0.37684324383735657,\n",
       "  0.368569552898407,\n",
       "  0.3708687722682953,\n",
       "  0.3690885901451111,\n",
       "  0.37171873450279236,\n",
       "  0.3721287250518799,\n",
       "  0.3712027966976166,\n",
       "  0.36764347553253174,\n",
       "  0.3710545599460602,\n",
       "  0.37425002455711365,\n",
       "  0.36947184801101685,\n",
       "  0.3681257665157318,\n",
       "  0.3703506290912628,\n",
       "  0.3659825325012207,\n",
       "  0.3635871410369873,\n",
       "  0.3646479845046997,\n",
       "  0.3659554421901703,\n",
       "  0.3713926374912262,\n",
       "  0.37073689699172974,\n",
       "  0.36524274945259094,\n",
       "  0.36190065741539,\n",
       "  0.3737134635448456,\n",
       "  0.3611946105957031,\n",
       "  0.36612755060195923,\n",
       "  0.3654777407646179,\n",
       "  0.3622852861881256,\n",
       "  0.36318477988243103,\n",
       "  0.36179664731025696,\n",
       "  0.3639948070049286,\n",
       "  0.3665631413459778,\n",
       "  0.3640001714229584,\n",
       "  0.3601559102535248,\n",
       "  0.3619096875190735,\n",
       "  0.3614697754383087,\n",
       "  0.36001041531562805,\n",
       "  0.3619649410247803,\n",
       "  0.36222878098487854,\n",
       "  0.3619266748428345,\n",
       "  0.3607237935066223,\n",
       "  0.36455053091049194,\n",
       "  0.3590327501296997,\n",
       "  0.3568458557128906,\n",
       "  0.35802987217903137,\n",
       "  0.3608046770095825,\n",
       "  0.3611483573913574,\n",
       "  0.36359670758247375,\n",
       "  0.3654618561267853,\n",
       "  0.35411885380744934,\n",
       "  0.3582583963871002,\n",
       "  0.36061134934425354,\n",
       "  0.3563244640827179,\n",
       "  0.36044391989707947,\n",
       "  0.36009013652801514,\n",
       "  0.36222562193870544,\n",
       "  0.3576686680316925,\n",
       "  0.3553324341773987,\n",
       "  0.35805028676986694,\n",
       "  0.35457223653793335,\n",
       "  0.35420098900794983,\n",
       "  0.35896822810173035,\n",
       "  0.35661086440086365,\n",
       "  0.3535643517971039,\n",
       "  0.35768693685531616,\n",
       "  0.3518896698951721],\n",
       " 'accuracy': [0.6004993915557861,\n",
       "  0.6192259788513184,\n",
       "  0.7253433465957642,\n",
       "  0.7602996230125427,\n",
       "  0.7802746295928955,\n",
       "  0.7765293121337891,\n",
       "  0.7965043783187866,\n",
       "  0.8039950132369995,\n",
       "  0.807740330696106,\n",
       "  0.802746593952179,\n",
       "  0.8002496957778931,\n",
       "  0.802746593952179,\n",
       "  0.8114856481552124,\n",
       "  0.8239700198173523,\n",
       "  0.8039950132369995,\n",
       "  0.8214731812477112,\n",
       "  0.8139825463294983,\n",
       "  0.8189762830734253,\n",
       "  0.8189762830734253,\n",
       "  0.8214731812477112,\n",
       "  0.8164793848991394,\n",
       "  0.8227216005325317,\n",
       "  0.8239700198173523,\n",
       "  0.8239700198173523,\n",
       "  0.8277153372764587,\n",
       "  0.8339575529098511,\n",
       "  0.8152309656143188,\n",
       "  0.8239700198173523,\n",
       "  0.8214731812477112,\n",
       "  0.8139825463294983,\n",
       "  0.8289638161659241,\n",
       "  0.8302122354507446,\n",
       "  0.8214731812477112,\n",
       "  0.8189762830734253,\n",
       "  0.8314606547355652,\n",
       "  0.8264669179916382,\n",
       "  0.8277153372764587,\n",
       "  0.8327091336250305,\n",
       "  0.8302122354507446,\n",
       "  0.8189762830734253,\n",
       "  0.8252184987068176,\n",
       "  0.8214731812477112,\n",
       "  0.8214731812477112,\n",
       "  0.8401997685432434,\n",
       "  0.8302122354507446,\n",
       "  0.8302122354507446,\n",
       "  0.8264669179916382,\n",
       "  0.8352059721946716,\n",
       "  0.8327091336250305,\n",
       "  0.8252184987068176,\n",
       "  0.8264669179916382,\n",
       "  0.8314606547355652,\n",
       "  0.8302122354507446,\n",
       "  0.8314606547355652,\n",
       "  0.8302122354507446,\n",
       "  0.8277153372764587,\n",
       "  0.8327091336250305,\n",
       "  0.8302122354507446,\n",
       "  0.8189762830734253,\n",
       "  0.8277153372764587,\n",
       "  0.8264669179916382,\n",
       "  0.8339575529098511,\n",
       "  0.8352059721946716,\n",
       "  0.8164793848991394,\n",
       "  0.8227216005325317,\n",
       "  0.8327091336250305,\n",
       "  0.8339575529098511,\n",
       "  0.8164793848991394,\n",
       "  0.8339575529098511,\n",
       "  0.8401997685432434,\n",
       "  0.8277153372764587,\n",
       "  0.8339575529098511,\n",
       "  0.8302122354507446,\n",
       "  0.8389512896537781,\n",
       "  0.8389512896537781,\n",
       "  0.8339575529098511,\n",
       "  0.8314606547355652,\n",
       "  0.8239700198173523,\n",
       "  0.8277153372764587,\n",
       "  0.8289638161659241,\n",
       "  0.8389512896537781,\n",
       "  0.8289638161659241,\n",
       "  0.8264669179916382,\n",
       "  0.8401997685432434,\n",
       "  0.8289638161659241,\n",
       "  0.836454451084137,\n",
       "  0.8277153372764587,\n",
       "  0.8339575529098511,\n",
       "  0.8202247023582458,\n",
       "  0.8277153372764587,\n",
       "  0.836454451084137,\n",
       "  0.8352059721946716,\n",
       "  0.8289638161659241,\n",
       "  0.8302122354507446,\n",
       "  0.8327091336250305,\n",
       "  0.8327091336250305,\n",
       "  0.8352059721946716,\n",
       "  0.8289638161659241,\n",
       "  0.8389512896537781,\n",
       "  0.8302122354507446,\n",
       "  0.8289638161659241,\n",
       "  0.8289638161659241,\n",
       "  0.8327091336250305,\n",
       "  0.8289638161659241,\n",
       "  0.836454451084137,\n",
       "  0.8339575529098511,\n",
       "  0.836454451084137,\n",
       "  0.8302122354507446,\n",
       "  0.8277153372764587,\n",
       "  0.8377028703689575,\n",
       "  0.841448187828064,\n",
       "  0.836454451084137,\n",
       "  0.8302122354507446,\n",
       "  0.8327091336250305,\n",
       "  0.8352059721946716,\n",
       "  0.841448187828064,\n",
       "  0.836454451084137,\n",
       "  0.8289638161659241,\n",
       "  0.8314606547355652,\n",
       "  0.8327091336250305,\n",
       "  0.8339575529098511,\n",
       "  0.8327091336250305,\n",
       "  0.8239700198173523,\n",
       "  0.8401997685432434,\n",
       "  0.8377028703689575,\n",
       "  0.8289638161659241,\n",
       "  0.8302122354507446,\n",
       "  0.836454451084137,\n",
       "  0.8377028703689575,\n",
       "  0.8314606547355652,\n",
       "  0.8227216005325317,\n",
       "  0.8352059721946716,\n",
       "  0.8227216005325317,\n",
       "  0.8264669179916382,\n",
       "  0.8289638161659241,\n",
       "  0.8302122354507446,\n",
       "  0.8264669179916382,\n",
       "  0.8302122354507446,\n",
       "  0.8339575529098511,\n",
       "  0.8239700198173523,\n",
       "  0.836454451084137,\n",
       "  0.8327091336250305,\n",
       "  0.8352059721946716,\n",
       "  0.8401997685432434,\n",
       "  0.8451935052871704,\n",
       "  0.8352059721946716,\n",
       "  0.8352059721946716,\n",
       "  0.8339575529098511,\n",
       "  0.8426966071128845,\n",
       "  0.836454451084137,\n",
       "  0.8352059721946716,\n",
       "  0.8327091336250305,\n",
       "  0.8289638161659241,\n",
       "  0.8377028703689575,\n",
       "  0.8264669179916382,\n",
       "  0.8439450860023499,\n",
       "  0.8302122354507446,\n",
       "  0.8352059721946716,\n",
       "  0.8339575529098511,\n",
       "  0.8277153372764587,\n",
       "  0.841448187828064,\n",
       "  0.8377028703689575,\n",
       "  0.8302122354507446,\n",
       "  0.8377028703689575,\n",
       "  0.8401997685432434,\n",
       "  0.8339575529098511,\n",
       "  0.8426966071128845,\n",
       "  0.8314606547355652,\n",
       "  0.8239700198173523,\n",
       "  0.8327091336250305,\n",
       "  0.8426966071128845,\n",
       "  0.8352059721946716,\n",
       "  0.8314606547355652,\n",
       "  0.8401997685432434,\n",
       "  0.8426966071128845,\n",
       "  0.8389512896537781,\n",
       "  0.8352059721946716,\n",
       "  0.8302122354507446,\n",
       "  0.8439450860023499,\n",
       "  0.8401997685432434,\n",
       "  0.841448187828064,\n",
       "  0.8426966071128845,\n",
       "  0.8401997685432434,\n",
       "  0.8426966071128845,\n",
       "  0.8401997685432434,\n",
       "  0.8377028703689575,\n",
       "  0.8302122354507446,\n",
       "  0.8314606547355652,\n",
       "  0.8314606547355652,\n",
       "  0.841448187828064,\n",
       "  0.8439450860023499,\n",
       "  0.8389512896537781,\n",
       "  0.8377028703689575,\n",
       "  0.8401997685432434,\n",
       "  0.8426966071128845,\n",
       "  0.8426966071128845,\n",
       "  0.8389512896537781,\n",
       "  0.8401997685432434,\n",
       "  0.8401997685432434,\n",
       "  0.8401997685432434,\n",
       "  0.8377028703689575,\n",
       "  0.8314606547355652,\n",
       "  0.8389512896537781,\n",
       "  0.8426966071128845,\n",
       "  0.8426966071128845,\n",
       "  0.8426966071128845,\n",
       "  0.8401997685432434,\n",
       "  0.8339575529098511,\n",
       "  0.8439450860023499,\n",
       "  0.8439450860023499,\n",
       "  0.8389512896537781,\n",
       "  0.841448187828064,\n",
       "  0.8339575529098511,\n",
       "  0.836454451084137,\n",
       "  0.8426966071128845,\n",
       "  0.841448187828064,\n",
       "  0.8439450860023499,\n",
       "  0.8339575529098511,\n",
       "  0.8352059721946716,\n",
       "  0.8339575529098511,\n",
       "  0.8401997685432434,\n",
       "  0.836454451084137,\n",
       "  0.8401997685432434,\n",
       "  0.8401997685432434,\n",
       "  0.8439450860023499,\n",
       "  0.8401997685432434,\n",
       "  0.8476904034614563,\n",
       "  0.8451935052871704,\n",
       "  0.8439450860023499,\n",
       "  0.841448187828064,\n",
       "  0.8377028703689575,\n",
       "  0.841448187828064,\n",
       "  0.8451935052871704,\n",
       "  0.8489388227462769,\n",
       "  0.8426966071128845,\n",
       "  0.8426966071128845,\n",
       "  0.8352059721946716,\n",
       "  0.836454451084137,\n",
       "  0.8339575529098511,\n",
       "  0.8389512896537781,\n",
       "  0.8377028703689575,\n",
       "  0.8439450860023499,\n",
       "  0.8352059721946716,\n",
       "  0.8439450860023499,\n",
       "  0.8451935052871704,\n",
       "  0.836454451084137,\n",
       "  0.841448187828064,\n",
       "  0.846441924571991,\n",
       "  0.8377028703689575,\n",
       "  0.8377028703689575,\n",
       "  0.846441924571991,\n",
       "  0.8539325594902039,\n",
       "  0.8439450860023499,\n",
       "  0.836454451084137,\n",
       "  0.8377028703689575,\n",
       "  0.8439450860023499,\n",
       "  0.8439450860023499,\n",
       "  0.8352059721946716,\n",
       "  0.8451935052871704,\n",
       "  0.8401997685432434,\n",
       "  0.841448187828064,\n",
       "  0.8451935052871704,\n",
       "  0.8426966071128845,\n",
       "  0.8439450860023499,\n",
       "  0.8426966071128845,\n",
       "  0.8401997685432434,\n",
       "  0.8439450860023499,\n",
       "  0.8451935052871704,\n",
       "  0.8377028703689575,\n",
       "  0.841448187828064,\n",
       "  0.8476904034614563,\n",
       "  0.8451935052871704,\n",
       "  0.836454451084137,\n",
       "  0.8401997685432434,\n",
       "  0.8401997685432434,\n",
       "  0.8439450860023499,\n",
       "  0.8439450860023499,\n",
       "  0.8489388227462769,\n",
       "  0.8576778769493103,\n",
       "  0.8389512896537781,\n",
       "  0.8489388227462769,\n",
       "  0.841448187828064,\n",
       "  0.8439450860023499,\n",
       "  0.8451935052871704,\n",
       "  0.8476904034614563,\n",
       "  0.8451935052871704,\n",
       "  0.846441924571991,\n",
       "  0.8451935052871704,\n",
       "  0.8339575529098511,\n",
       "  0.8451935052871704,\n",
       "  0.846441924571991,\n",
       "  0.8551810383796692,\n",
       "  0.8476904034614563,\n",
       "  0.8451935052871704,\n",
       "  0.8501872420310974,\n",
       "  0.8476904034614563,\n",
       "  0.8439450860023499,\n",
       "  0.8489388227462769,\n",
       "  0.846441924571991,\n",
       "  0.8526841402053833],\n",
       " 'val_loss': [0.6551007032394409,\n",
       "  0.5781837701797485,\n",
       "  0.47712352871894836,\n",
       "  0.44496726989746094,\n",
       "  0.42673972249031067,\n",
       "  0.4276130199432373,\n",
       "  0.4204678535461426,\n",
       "  0.4038093090057373,\n",
       "  0.4055616557598114,\n",
       "  0.41033029556274414,\n",
       "  0.3993246853351593,\n",
       "  0.39373669028282166,\n",
       "  0.39159998297691345,\n",
       "  0.388253778219223,\n",
       "  0.38498470187187195,\n",
       "  0.38099297881126404,\n",
       "  0.39055100083351135,\n",
       "  0.39544618129730225,\n",
       "  0.4054837226867676,\n",
       "  0.37725338339805603,\n",
       "  0.3781445324420929,\n",
       "  0.37161773443222046,\n",
       "  0.3700391948223114,\n",
       "  0.3665540814399719,\n",
       "  0.4032995402812958,\n",
       "  0.3734349012374878,\n",
       "  0.36663439869880676,\n",
       "  0.36550503969192505,\n",
       "  0.3793621361255646,\n",
       "  0.3624582588672638,\n",
       "  0.3622373938560486,\n",
       "  0.36212149262428284,\n",
       "  0.3708212971687317,\n",
       "  0.36247608065605164,\n",
       "  0.3606099486351013,\n",
       "  0.3559199869632721,\n",
       "  0.35744133591651917,\n",
       "  0.3856411874294281,\n",
       "  0.40766090154647827,\n",
       "  0.35810771584510803,\n",
       "  0.3753606677055359,\n",
       "  0.40209782123565674,\n",
       "  0.35227736830711365,\n",
       "  0.3689700663089752,\n",
       "  0.38843950629234314,\n",
       "  0.35792529582977295,\n",
       "  0.3630896806716919,\n",
       "  0.37603792548179626,\n",
       "  0.3554195761680603,\n",
       "  0.35918042063713074,\n",
       "  0.35553643107414246,\n",
       "  0.34992697834968567,\n",
       "  0.39878472685813904,\n",
       "  0.36133769154548645,\n",
       "  0.3790627419948578,\n",
       "  0.36840498447418213,\n",
       "  0.35565057396888733,\n",
       "  0.3537340462207794,\n",
       "  0.36252737045288086,\n",
       "  0.3749374747276306,\n",
       "  0.3597915768623352,\n",
       "  0.3540126383304596,\n",
       "  0.37705984711647034,\n",
       "  0.36233723163604736,\n",
       "  0.374545156955719,\n",
       "  0.3614344000816345,\n",
       "  0.35383141040802,\n",
       "  0.35487937927246094,\n",
       "  0.36345160007476807,\n",
       "  0.4450311064720154,\n",
       "  0.39352306723594666,\n",
       "  0.35059112310409546,\n",
       "  0.35915443301200867,\n",
       "  0.3833931088447571,\n",
       "  0.3560371398925781,\n",
       "  0.3537214696407318,\n",
       "  0.37809279561042786,\n",
       "  0.37360402941703796,\n",
       "  0.3591171205043793,\n",
       "  0.38197505474090576,\n",
       "  0.3789098560810089,\n",
       "  0.3677031099796295,\n",
       "  0.3527176082134247,\n",
       "  0.35990452766418457,\n",
       "  0.3683846890926361,\n",
       "  0.3632749021053314,\n",
       "  0.3560611605644226,\n",
       "  0.3711245357990265,\n",
       "  0.36655664443969727,\n",
       "  0.3656004071235657,\n",
       "  0.3866097033023834,\n",
       "  0.3580637276172638,\n",
       "  0.36924853920936584,\n",
       "  0.3642342984676361,\n",
       "  0.36962348222732544,\n",
       "  0.36488664150238037,\n",
       "  0.35297390818595886,\n",
       "  0.3561776876449585,\n",
       "  0.3677386939525604,\n",
       "  0.375249445438385,\n",
       "  0.37306705117225647,\n",
       "  0.3742530941963196,\n",
       "  0.3923988342285156,\n",
       "  0.3715219795703888,\n",
       "  0.3801421821117401,\n",
       "  0.35831981897354126,\n",
       "  0.3821127116680145,\n",
       "  0.3680606484413147,\n",
       "  0.3955861032009125,\n",
       "  0.36796510219573975,\n",
       "  0.3814104199409485,\n",
       "  0.3709454834461212,\n",
       "  0.3757281005382538,\n",
       "  0.36510518193244934,\n",
       "  0.3751329183578491,\n",
       "  0.3948499858379364,\n",
       "  0.3671165406703949,\n",
       "  0.36202242970466614,\n",
       "  0.37720853090286255,\n",
       "  0.36209896206855774,\n",
       "  0.39111196994781494,\n",
       "  0.4304159879684448,\n",
       "  0.38896629214286804,\n",
       "  0.3781789541244507,\n",
       "  0.3979424238204956,\n",
       "  0.3570061922073364,\n",
       "  0.3897976875305176,\n",
       "  0.3669818937778473,\n",
       "  0.3834106922149658,\n",
       "  0.36893343925476074,\n",
       "  0.3874959349632263,\n",
       "  0.37688639760017395,\n",
       "  0.4006848931312561,\n",
       "  0.37341803312301636,\n",
       "  0.4031214416027069,\n",
       "  0.380511611700058,\n",
       "  0.3555662930011749,\n",
       "  0.3700881600379944,\n",
       "  0.4171040952205658,\n",
       "  0.37265995144844055,\n",
       "  0.4138512909412384,\n",
       "  0.376268208026886,\n",
       "  0.35183829069137573,\n",
       "  0.4056415855884552,\n",
       "  0.36514756083488464,\n",
       "  0.3633366823196411,\n",
       "  0.4577212333679199,\n",
       "  0.3702310025691986,\n",
       "  0.3603915274143219,\n",
       "  0.3560863435268402,\n",
       "  0.3573562204837799,\n",
       "  0.36071789264678955,\n",
       "  0.3478107750415802,\n",
       "  0.3801093101501465,\n",
       "  0.40572184324264526,\n",
       "  0.3738566040992737,\n",
       "  0.38229280710220337,\n",
       "  0.41362419724464417,\n",
       "  0.38750821352005005,\n",
       "  0.3626856505870819,\n",
       "  0.37739089131355286,\n",
       "  0.35983937978744507,\n",
       "  0.36754530668258667,\n",
       "  0.3859199583530426,\n",
       "  0.390746146440506,\n",
       "  0.3690530061721802,\n",
       "  0.4094389081001282,\n",
       "  0.3699253797531128,\n",
       "  0.3748176693916321,\n",
       "  0.36722421646118164,\n",
       "  0.370410293340683,\n",
       "  0.3586116433143616,\n",
       "  0.36404040455818176,\n",
       "  0.37066853046417236,\n",
       "  0.38869184255599976,\n",
       "  0.36503541469573975,\n",
       "  0.38862699270248413,\n",
       "  0.37586331367492676,\n",
       "  0.3769982159137726,\n",
       "  0.3658100664615631,\n",
       "  0.362416535615921,\n",
       "  0.36238542199134827,\n",
       "  0.42245182394981384,\n",
       "  0.39247754216194153,\n",
       "  0.36957523226737976,\n",
       "  0.35269346833229065,\n",
       "  0.3714459240436554,\n",
       "  0.3598233461380005,\n",
       "  0.34836921095848083,\n",
       "  0.3626447319984436,\n",
       "  0.369047075510025,\n",
       "  0.40011516213417053,\n",
       "  0.3841213285923004,\n",
       "  0.3878767192363739,\n",
       "  0.3698580861091614,\n",
       "  0.39344778656959534,\n",
       "  0.3711073696613312,\n",
       "  0.3833875060081482,\n",
       "  0.3677089810371399,\n",
       "  0.38674426078796387,\n",
       "  0.413112610578537,\n",
       "  0.36035987734794617,\n",
       "  0.4067840576171875,\n",
       "  0.3784450888633728,\n",
       "  0.39801594614982605,\n",
       "  0.3624846935272217,\n",
       "  0.3599647581577301,\n",
       "  0.3599144518375397,\n",
       "  0.39347586035728455,\n",
       "  0.39637356996536255,\n",
       "  0.3687431216239929,\n",
       "  0.37159138917922974,\n",
       "  0.38952869176864624,\n",
       "  0.3845616579055786,\n",
       "  0.40406176447868347,\n",
       "  0.36662566661834717,\n",
       "  0.3829611539840698,\n",
       "  0.3740190863609314,\n",
       "  0.4117062985897064,\n",
       "  0.37391144037246704,\n",
       "  0.3866104483604431,\n",
       "  0.36206087470054626,\n",
       "  0.37621045112609863,\n",
       "  0.37396734952926636,\n",
       "  0.38584083318710327,\n",
       "  0.3712335228919983,\n",
       "  0.39612358808517456,\n",
       "  0.3685391843318939,\n",
       "  0.3623259365558624,\n",
       "  0.3712497651576996,\n",
       "  0.38526949286460876,\n",
       "  0.37673234939575195,\n",
       "  0.3842111825942993,\n",
       "  0.37518391013145447,\n",
       "  0.40161964297294617,\n",
       "  0.38804885745048523,\n",
       "  0.3757748603820801,\n",
       "  0.39709851145744324,\n",
       "  0.38552233576774597,\n",
       "  0.37969228625297546,\n",
       "  0.369912713766098,\n",
       "  0.3641057312488556,\n",
       "  0.38432154059410095,\n",
       "  0.3744494616985321,\n",
       "  0.3729031980037689,\n",
       "  0.38865670561790466,\n",
       "  0.3881429433822632,\n",
       "  0.4024656116962433,\n",
       "  0.39603984355926514,\n",
       "  0.380541056394577,\n",
       "  0.3877200782299042,\n",
       "  0.40491384267807007,\n",
       "  0.38736963272094727,\n",
       "  0.37480926513671875,\n",
       "  0.409731388092041,\n",
       "  0.3793019950389862,\n",
       "  0.4633677303791046,\n",
       "  0.38674256205558777,\n",
       "  0.37040892243385315,\n",
       "  0.40803465247154236,\n",
       "  0.40112483501434326,\n",
       "  0.42346489429473877,\n",
       "  0.4080107808113098,\n",
       "  0.40703433752059937,\n",
       "  0.42487725615501404,\n",
       "  0.41273176670074463,\n",
       "  0.38210347294807434,\n",
       "  0.42566511034965515,\n",
       "  0.4111214578151703,\n",
       "  0.3919556438922882,\n",
       "  0.41157564520835876,\n",
       "  0.38924354314804077,\n",
       "  0.4047415852546692,\n",
       "  0.3789351284503937,\n",
       "  0.40766584873199463,\n",
       "  0.41655638813972473,\n",
       "  0.39420953392982483,\n",
       "  0.4291384816169739,\n",
       "  0.41209113597869873,\n",
       "  0.37164968252182007,\n",
       "  0.4514907896518707,\n",
       "  0.3995804786682129,\n",
       "  0.40785616636276245,\n",
       "  0.4008437693119049,\n",
       "  0.43232086300849915,\n",
       "  0.39971834421157837,\n",
       "  0.4363555908203125,\n",
       "  0.4319744408130646,\n",
       "  0.41442617774009705,\n",
       "  0.4604659080505371,\n",
       "  0.4639818072319031,\n",
       "  0.4150741696357727,\n",
       "  0.41963720321655273,\n",
       "  0.43357986211776733,\n",
       "  0.4846285581588745,\n",
       "  0.4123390316963196,\n",
       "  0.4101499021053314,\n",
       "  0.4313010275363922,\n",
       "  0.3892292380332947,\n",
       "  0.424957811832428],\n",
       " 'val_accuracy': [0.6222222447395325,\n",
       "  0.6222222447395325,\n",
       "  0.800000011920929,\n",
       "  0.7888888716697693,\n",
       "  0.8111110925674438,\n",
       "  0.7777777910232544,\n",
       "  0.7888888716697693,\n",
       "  0.8222222328186035,\n",
       "  0.8222222328186035,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8111110925674438,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8222222328186035,\n",
       "  0.8222222328186035,\n",
       "  0.8222222328186035,\n",
       "  0.8222222328186035,\n",
       "  0.8111110925674438,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8222222328186035,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8444444537162781,\n",
       "  0.8333333134651184,\n",
       "  0.8666666746139526,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.855555534362793,\n",
       "  0.855555534362793,\n",
       "  0.8444444537162781,\n",
       "  0.8666666746139526,\n",
       "  0.8333333134651184,\n",
       "  0.8222222328186035,\n",
       "  0.8111110925674438,\n",
       "  0.8444444537162781,\n",
       "  0.8333333134651184,\n",
       "  0.8222222328186035,\n",
       "  0.855555534362793,\n",
       "  0.8444444537162781,\n",
       "  0.8111110925674438,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.855555534362793,\n",
       "  0.8777777552604675,\n",
       "  0.8444444537162781,\n",
       "  0.8666666746139526,\n",
       "  0.8111110925674438,\n",
       "  0.8777777552604675,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8333333134651184,\n",
       "  0.8444444537162781,\n",
       "  0.855555534362793,\n",
       "  0.855555534362793,\n",
       "  0.8333333134651184,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8777777552604675,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.800000011920929,\n",
       "  0.8222222328186035,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8222222328186035,\n",
       "  0.8666666746139526,\n",
       "  0.855555534362793,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8777777552604675,\n",
       "  0.800000011920929,\n",
       "  0.8333333134651184,\n",
       "  0.8666666746139526,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8333333134651184,\n",
       "  0.8444444537162781,\n",
       "  0.855555534362793,\n",
       "  0.8222222328186035,\n",
       "  0.8444444537162781,\n",
       "  0.855555534362793,\n",
       "  0.8333333134651184,\n",
       "  0.8444444537162781,\n",
       "  0.8333333134651184,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8222222328186035,\n",
       "  0.855555534362793,\n",
       "  0.855555534362793,\n",
       "  0.8444444537162781,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.855555534362793,\n",
       "  0.855555534362793,\n",
       "  0.855555534362793,\n",
       "  0.8222222328186035,\n",
       "  0.8444444537162781,\n",
       "  0.8333333134651184,\n",
       "  0.855555534362793,\n",
       "  0.8444444537162781,\n",
       "  0.8777777552604675,\n",
       "  0.8333333134651184,\n",
       "  0.8777777552604675,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.855555534362793,\n",
       "  0.855555534362793,\n",
       "  0.8333333134651184,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8222222328186035,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8444444537162781,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.855555534362793,\n",
       "  0.855555534362793,\n",
       "  0.855555534362793,\n",
       "  0.8666666746139526,\n",
       "  0.800000011920929,\n",
       "  0.855555534362793,\n",
       "  0.8444444537162781,\n",
       "  0.8666666746139526,\n",
       "  0.855555534362793,\n",
       "  0.855555534362793,\n",
       "  0.8111110925674438,\n",
       "  0.855555534362793,\n",
       "  0.8222222328186035,\n",
       "  0.8666666746139526,\n",
       "  0.855555534362793,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.855555534362793,\n",
       "  0.7888888716697693,\n",
       "  0.8777777552604675,\n",
       "  0.8444444537162781,\n",
       "  0.855555534362793,\n",
       "  0.8666666746139526,\n",
       "  0.8444444537162781,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.800000011920929,\n",
       "  0.855555534362793,\n",
       "  0.8111110925674438,\n",
       "  0.8111110925674438,\n",
       "  0.8333333134651184,\n",
       "  0.855555534362793,\n",
       "  0.8333333134651184,\n",
       "  0.855555534362793,\n",
       "  0.8444444537162781,\n",
       "  0.8333333134651184,\n",
       "  0.855555534362793,\n",
       "  0.8666666746139526,\n",
       "  0.8333333134651184,\n",
       "  0.8444444537162781,\n",
       "  0.8333333134651184,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.855555534362793,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8333333134651184,\n",
       "  0.855555534362793,\n",
       "  0.8333333134651184,\n",
       "  0.8666666746139526,\n",
       "  0.855555534362793,\n",
       "  0.8666666746139526,\n",
       "  0.8777777552604675,\n",
       "  0.8444444537162781,\n",
       "  0.8222222328186035,\n",
       "  0.8333333134651184,\n",
       "  0.855555534362793,\n",
       "  0.8444444537162781,\n",
       "  0.8666666746139526,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.855555534362793,\n",
       "  0.8444444537162781,\n",
       "  0.855555534362793,\n",
       "  0.8666666746139526,\n",
       "  0.8444444537162781,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8444444537162781,\n",
       "  0.8666666746139526,\n",
       "  0.8444444537162781,\n",
       "  0.8666666746139526,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8444444537162781,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.855555534362793,\n",
       "  0.8444444537162781,\n",
       "  0.855555534362793,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8777777552604675,\n",
       "  0.8666666746139526,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8666666746139526,\n",
       "  0.855555534362793,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8333333134651184,\n",
       "  0.855555534362793,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8333333134651184,\n",
       "  0.8444444537162781,\n",
       "  0.8222222328186035,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8666666746139526,\n",
       "  0.8666666746139526,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.855555534362793,\n",
       "  0.8666666746139526,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8666666746139526,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8333333134651184,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8333333134651184,\n",
       "  0.8444444537162781,\n",
       "  0.8333333134651184,\n",
       "  0.8666666746139526,\n",
       "  0.8444444537162781,\n",
       "  0.8222222328186035,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.855555534362793,\n",
       "  0.8222222328186035,\n",
       "  0.8111110925674438,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.855555534362793,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.855555534362793,\n",
       "  0.8444444537162781,\n",
       "  0.855555534362793,\n",
       "  0.855555534362793,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8444444537162781,\n",
       "  0.8222222328186035,\n",
       "  0.855555534362793,\n",
       "  0.8333333134651184,\n",
       "  0.8111110925674438,\n",
       "  0.8222222328186035,\n",
       "  0.8222222328186035,\n",
       "  0.8222222328186035,\n",
       "  0.8444444537162781,\n",
       "  0.855555534362793,\n",
       "  0.8444444537162781,\n",
       "  0.8111110925674438,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781,\n",
       "  0.8444444537162781]}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-10453431ef2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0malg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mMLA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain1_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain1_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest1_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest1_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MLA' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "result['PassengerId'] = data_val['PassengerId']\n",
    "result['Survived'] = df\n",
    "result.to_csv('nn' +\".csv\",index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         0\n",
       "..           ...       ...\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\n",
    "sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n",
    "\n",
    "#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "plt.title('Machine Learning Algorithm Accuracy Score \\n')\n",
    "plt.xlabel('Accuracy Score (%)')\n",
    "plt.ylabel('Algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
